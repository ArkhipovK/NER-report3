Rows processed: 194000
Found 289161 unique tokens.
Train on 429696 samples, validate on 47745 samples
Epoch 1/30
2018-10-14 16:18:58.237684: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
429696/429696 [==============================] - 3273s 8ms/step - loss: -0.0084 - acc: 0.9982 - val_loss: -0.0238 - val_acc: 0.9982
Epoch 2/30
429696/429696 [==============================] - 3011s 7ms/step - loss: -0.0391 - acc: 0.9982 - val_loss: -0.0532 - val_acc: 0.9982
Epoch 3/30
429696/429696 [==============================] - 3012s 7ms/step - loss: -0.0688 - acc: 0.9982 - val_loss: -0.0846 - val_acc: 0.9982
Epoch 4/30
429696/429696 [==============================] - 3010s 7ms/step - loss: -0.0980 - acc: 0.9982 - val_loss: -0.1142 - val_acc: 0.9982
Epoch 5/30
429696/429696 [==============================] - 3005s 7ms/step - loss: -0.1267 - acc: 0.9982 - val_loss: -0.1389 - val_acc: 0.9982
Epoch 6/30
429696/429696 [==============================] - 3017s 7ms/step - loss: -0.1551 - acc: 0.9982 - val_loss: -0.1688 - val_acc: 0.9983
Epoch 7/30
429696/429696 [==============================] - 3011s 7ms/step - loss: -0.1832 - acc: 0.9982 - val_loss: -0.1972 - val_acc: 0.9983
Epoch 8/30
429696/429696 [==============================] - 3021s 7ms/step - loss: -0.2112 - acc: 0.9982 - val_loss: -0.2248 - val_acc: 0.9983
Epoch 9/30
429696/429696 [==============================] - 3019s 7ms/step - loss: -0.2390 - acc: 0.9982 - val_loss: -0.2522 - val_acc: 0.9983
Epoch 10/30
429696/429696 [==============================] - 3003s 7ms/step - loss: -0.2665 - acc: 0.9982 - val_loss: -0.2807 - val_acc: 0.9983
Epoch 11/30
429696/429696 [==============================] - 3017s 7ms/step - loss: -0.2939 - acc: 0.9982 - val_loss: -0.3077 - val_acc: 0.9983
Epoch 12/30
429696/429696 [==============================] - 3006s 7ms/step - loss: -0.3212 - acc: 0.9982 - val_loss: -0.3339 - val_acc: 0.9983
Epoch 13/30
429696/429696 [==============================] - 2991s 7ms/step - loss: -0.3483 - acc: 0.9982 - val_loss: -0.3616 - val_acc: 0.9983
Epoch 14/30
429696/429696 [==============================] - 2984s 7ms/step - loss: -0.3753 - acc: 0.9982 - val_loss: -0.3884 - val_acc: 0.9983
Epoch 15/30
429696/429696 [==============================] - 2979s 7ms/step - loss: -0.4022 - acc: 0.9982 - val_loss: -0.4163 - val_acc: 0.9983
Epoch 16/30
429696/429696 [==============================] - 2975s 7ms/step - loss: -0.4289 - acc: 0.9982 - val_loss: -0.4391 - val_acc: 0.9979
Epoch 17/30
429696/429696 [==============================] - 2980s 7ms/step - loss: -0.4555 - acc: 0.9982 - val_loss: -0.4693 - val_acc: 0.9982
Epoch 18/30
429696/429696 [==============================] - 2983s 7ms/step - loss: -0.4820 - acc: 0.9982 - val_loss: -0.4948 - val_acc: 0.9981
Epoch 19/30
429696/429696 [==============================] - 3008s 7ms/step - loss: -0.5082 - acc: 0.9982 - val_loss: -0.5220 - val_acc: 0.9983
Epoch 20/30
429696/429696 [==============================] - 2978s 7ms/step - loss: -0.5345 - acc: 0.9982 - val_loss: -0.5466 - val_acc: 0.9982
Epoch 21/30
429696/429696 [==============================] - 2971s 7ms/step - loss: -0.5606 - acc: 0.9982 - val_loss: -0.5693 - val_acc: 0.9975
Epoch 22/30
429696/429696 [==============================] - 2971s 7ms/step - loss: -0.5865 - acc: 0.9982 - val_loss: -0.6007 - val_acc: 0.9982
Epoch 23/30
429696/429696 [==============================] - 2979s 7ms/step - loss: -0.6124 - acc: 0.9982 - val_loss: -0.6249 - val_acc: 0.9983
Epoch 24/30
429696/429696 [==============================] - 3133s 7ms/step - loss: -0.6380 - acc: 0.9982 - val_loss: -0.6515 - val_acc: 0.9983
Epoch 25/30
429696/429696 [==============================] - 3178s 7ms/step - loss: -0.6637 - acc: 0.9982 - val_loss: -0.6769 - val_acc: 0.9983
Epoch 26/30
429696/429696 [==============================] - 3190s 7ms/step - loss: -0.6892 - acc: 0.9982 - val_loss: -0.6990 - val_acc: 0.9980
Epoch 27/30
429696/429696 [==============================] - 3230s 8ms/step - loss: -0.7146 - acc: 0.9982 - val_loss: -0.7276 - val_acc: 0.9983
Epoch 28/30
429696/429696 [==============================] - 3228s 8ms/step - loss: -0.7399 - acc: 0.9982 - val_loss: -0.7535 - val_acc: 0.9983
Epoch 29/30
429696/429696 [==============================] - 7458s 17ms/step - loss: -0.7652 - acc: 0.9982 - val_loss: -0.7784 - val_acc: 0.9983
Epoch 30/30
429696/429696 [==============================] - 6550s 15ms/step - loss: -0.7904 - acc: 0.9982 - val_loss: -0.8027 - val_acc: 0.9983
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 327)               0
_________________________________________________________________
embedding_1 (Embedding)      (None, 327, 100)          28916200
_________________________________________________________________
bidirectional_1 (Bidirection (None, 327, 100)          60400
_________________________________________________________________
time_distributed_1 (TimeDist (None, 327, 50)           5050
_________________________________________________________________
crf_1 (CRF)                  (None, 327, 2)            110
=================================================================
Total params: 28,981,760
Trainable params: 65,560
Non-trainable params: 28,916,200
_________________________________________________________________
    val_loss   val_acc      loss       acc
0  -0.023830  0.998200 -0.008364  0.998192
1  -0.053219  0.998229 -0.039053  0.998212
2  -0.084595  0.998232 -0.068818  0.998220
3  -0.114155  0.998234 -0.097999  0.998221
4  -0.138917  0.998236 -0.126675  0.998228
5  -0.168838  0.998257 -0.155085  0.998227
6  -0.197190  0.998254 -0.183236  0.998231
7  -0.224849  0.998255 -0.211180  0.998231
8  -0.252194  0.998260 -0.238960  0.998230
9  -0.280715  0.998261 -0.266535  0.998233
10 -0.307653  0.998252 -0.293916  0.998234
11 -0.333932  0.998262 -0.321159  0.998232
12 -0.361586  0.998259 -0.348270  0.998229
13 -0.388411  0.998258 -0.375316  0.998233
14 -0.416316  0.998267 -0.402174  0.998231
15 -0.439106  0.997910 -0.428929  0.998230
16 -0.469253  0.998247 -0.455511  0.998233
17 -0.494775  0.998121 -0.481972  0.998235
18 -0.522027  0.998261 -0.508216  0.998224
19 -0.546621  0.998245 -0.534486  0.998232
20 -0.569263  0.997478 -0.560584  0.998231
21 -0.600732  0.998244 -0.586497  0.998230
22 -0.624851  0.998262 -0.612377  0.998231
23 -0.651476  0.998274 -0.637984  0.998223
24 -0.676886  0.998254 -0.663712  0.998228
25 -0.699011  0.998006 -0.689242  0.998226
26 -0.727636  0.998269 -0.714628  0.998222
27 -0.753527  0.998254 -0.739892  0.998217
28 -0.778382  0.998271 -0.765199  0.998224
29 -0.802730  0.998250 -0.790441  0.998223